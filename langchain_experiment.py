"""Markdown gradio experiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N8Kgpl3tlggmOTGtsqxR0x--oHW3i91y

When using conda I need to use `which` to be sure that I'm using the 
correct python and pip for installs. Sometimes I need to deactivate 
multiple times in order to get out of the base environment. Then I activate
the conda environment that I want and it uses the correct pip. See this link
https://github.com/ContinuumIO/anaconda-issues/issues/1429#issuecomment-1044871389

It helps to use the options python and pip when creating a new conda env
with the conda create command. 

Sometimes I need to clear browser cache and cookies in order to get mlflow ui
to load. 
"""
# Set the API key - if this key gets committed to a gitrepo then it gets
# disabled
import os

from langchain.chains import (
    StuffDocumentsChain,
    LLMChain,
    ReduceDocumentsChain,
    MapReduceDocumentsChain,
)
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.text_splitter import TokenTextSplitter
from langchain.document_loaders import YoutubeLoader

# from pyngrok import ngrok

import mlflow
import pandas as pd
import argparse

# In theory I should be able to set the parameters as environment vars
# so that I don't need to re-enter them every time...
# such as if I want to run this via the command line
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process API key and website")
    parser.add_argument(
        "--api-key", type=str, required=True, help="API key for the application"
    )
    parser.add_argument(
        "--website", type=str, required=True, help="Website to summarize"
    )
    args = parser.parse_args()

    # NGROK_AUTH_TOKEN = set_api_key('NGROK_AUTH_TOKEN',
    # 'Enter your ngrok auth token: ')
    # ngrok.set_auth_token(NGROK_AUTH_TOKEN)
    MY_API_KEY = args.api_key
    os.environ["OPENAI_API_KEY"] = MY_API_KEY

    website = args.website

# Instantiate the llm
llm = ChatOpenAI(model_name="gpt-3.5-turbo")

# Setting up the MapReduceDocumentsChain
# Setting up the map step with LLMChain
prompt_map = PromptTemplate.from_template("Summarize this content: {context}")
llm_chain_map = LLMChain(llm=llm, prompt=prompt_map)

# Setting up the reduce step
prompt_reduce = PromptTemplate.from_template(
    "Combine these summaries: {context}"
)
llm_chain_reduce = LLMChain(llm=llm, prompt=prompt_reduce)
combine_documents_chain = StuffDocumentsChain(
    llm_chain=llm_chain_reduce,
)
reduce_documents_chain = ReduceDocumentsChain(
    combine_documents_chain=combine_documents_chain,
    # The maximum number of tokens to group documents into.
    token_max=4000,
)

# Creating the MapReduceDocumentsChain
map_reduce_chain = MapReduceDocumentsChain(
    llm_chain=llm_chain_map, reduce_documents_chain=reduce_documents_chain
)

# Set up the text splitter and document loader
text_splitter = TokenTextSplitter(chunk_size=4000, chunk_overlap=0)

# Load the documents
loader = YoutubeLoader.from_youtube_url(
    website.strip(),
    add_video_info=True,
)
docs = loader.load_and_split(text_splitter=text_splitter)

# Set up the experiment tracking
mlflow.set_tracking_uri("")
experiment = mlflow.set_experiment("youtube_summarization")
with mlflow.start_run():
    # Log the number of docs
    print("Logging the number of docs")
    print(f'Num docs: {len(docs)}')
    params = {
        "num_docs": len(docs),
        "website": website,
    }
    mlflow.log_params(params)

    # Make prediction
    print()
    print('Making prediction...')
    inputs = [docs[0].page_content]
    outputs = [map_reduce_chain.run(docs)]
    prompts = [combine_documents_chain.llm_chain.prompt.template]

    print()
    print("ChatGPT output:")
    print(outputs[0])

    # Log prediction
    print()
    print("Logging prediction")
    model_info = mlflow.llm.log_predictions(inputs, outputs, prompts)

    # Logging the table artifacts
    print()
    print("Logging the table artifacts")
    data_dict = {
        "url": website,
        "model": "ChatGPT",
        "outputs": outputs,
    }
    df = pd.DataFrame(data_dict)
    mlflow.log_table(data=df, artifact_file="prediction_results.json")
